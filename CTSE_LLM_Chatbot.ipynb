{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af02ba1",
   "metadata": {},
   "source": [
    "# CTSE Lecture Notes Chatbot using LangChain + HuggingFace\n",
    "This notebook demonstrates a simple chatbot that answers questions based on CTSE lecture notes using a Retrieval-Augmented Generation (RAG) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-huggingface chromadb transformers unstructured sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500650f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bfe6c",
   "metadata": {},
   "source": [
    "# Step 1: Load the Lecture Notes PDF\n",
    "We'll use LangChain's `PyPDFLoader` to extract text from the CTSE lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data/ctse_notes.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"=\"*45)\n",
    "print(\"ðŸ“„ CTSE Lecture Notes Loaded Successfully\")\n",
    "print(\"=\"*45)\n",
    "print(f\"ðŸ“š Total Pages Loaded : {len(documents)}\")\n",
    "print(\"âœ… Source File        : data/ctse_notes.pdf\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd76fe",
   "metadata": {},
   "source": [
    "# Step 2: Split Documents into Chunks\n",
    "We use `RecursiveCharacterTextSplitter` to chunk the content into overlapping sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"=\"*45)\n",
    "print(\"ðŸ§© Document Chunking Completed\")\n",
    "print(\"=\"*45)\n",
    "print(f\"ðŸ”¹ Total Chunks Created : {len(docs)}\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3812fb",
   "metadata": {},
   "source": [
    "# Step 3: Generate Embeddings\n",
    "We'll use HuggingFace Sentence Transformers for vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea927201",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3cae08",
   "metadata": {},
   "source": [
    "# Step 4: Store Chunks in Chroma Vector DB\n",
    "We'll persist the chunks and embeddings to local vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0f91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./ctse_db\"\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embedding_model, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb4824",
   "metadata": {},
   "source": [
    "# Step 5: Load a Local LLM (FLAN-T5)\n",
    "We'll use HuggingFace's pipeline for open-source inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f96a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    tokenizer=\"google/flan-t5-base\",\n",
    "    max_length=512,\n",
    "    do_sample=False\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af863e87",
   "metadata": {},
   "source": [
    "# Step 6: Setup the QA Chain\n",
    "Combining retriever and LLM using LangChain's RetrievalQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f97e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f6a97",
   "metadata": {},
   "source": [
    "# Step 7: Ask Questions (Notebook-Friendly)\n",
    "You can ask any question based on the CTSE lecture notes.  \n",
    "Type `'exit'` to end the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_ctse_bot():\n",
    "    # Step 1: Introduction to the Chatbot\n",
    "    print(\"Welcome to the Interactive Chatbot! Ask questions based on the loaded PDF document.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Step 2: Take user input for the query\n",
    "        query = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
    "        \n",
    "        # Step 3: Exit condition\n",
    "        if query.lower().strip() == \"exit\":\n",
    "            print(\"Exiting the chatbot session. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Step 4: Try to retrieve the answer from the QA chain\n",
    "        try:\n",
    "            # Step 5: Retrieve the answer using the QA chain (which utilizes the retriever and language model)\n",
    "            answer = qa_chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Step 6: Print the question and the generated answer\n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            print(\"Your Question: \")\n",
    "            print(f\"{query}\\n\")\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "            print(\"Answer: \")\n",
    "            print(f\"{answer['result']}\\n\")\n",
    "            print(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        # Step 7: Handle any errors during the process\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing the query: {e}\")\n",
    "            print(\"Please try asking again.\\n\")\n",
    "\n",
    "# Step 8: Start the chatbot interaction\n",
    "interactive_ctse_bot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
