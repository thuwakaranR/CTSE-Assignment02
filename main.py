from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from transformers import pipeline

# âœ… Step 1: Load PDF documents
print("ğŸ“˜ Loading PDF...")
loader = PyPDFLoader("data/ctse_notes.pdf")
documents = loader.load()
print(f"ğŸ“„ Loaded {len(documents)} pages")

# âœ… Step 2: Split text into manageable chunks
print("âœ‚ï¸ Splitting documents into chunks...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
docs = text_splitter.split_documents(documents)
print(f"âœ… Total chunks created: {len(docs)}")

# âœ… Step 3: Generate embeddings using HuggingFace
print("ğŸ” Generating embeddings...")
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
print("âœ… Embedding model loaded successfully")

# âœ… Step 4: Store vectors in Chroma DB
persist_directory = "./ctse_db"
print("ğŸ’¾ Creating Chroma vector store...")
vectordb = Chroma.from_documents(
    documents=docs,
    embedding=embedding_model,
    persist_directory=persist_directory
)
print("ğŸ“¦ Vector store created and saved")

# âœ… Step 5: Load open-source LLM using HuggingFace Pipeline
print("ğŸ¤– Loading open-source model (google/flan-t5-base)...")
hf_pipeline = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    tokenizer="google/flan-t5-base",
    max_length=512,
    do_sample=False
)
llm = HuggingFacePipeline(pipeline=hf_pipeline)
print("âœ… LLM pipeline ready")

# âœ… Step 6: Set up Retrieval QA chain
print("ğŸ§  Setting up retrieval-based QA chain...")
retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
print("ğŸ”— RetrievalQA chain is ready")

# âœ… Step 7: Ask questions in a loop
while True:
    query = input("\nâ“ Ask a question (or type 'exit' to quit): ")
    if query.lower() == "exit":
        print("ğŸ‘‹ Exiting...")
        break
    answer = qa_chain.invoke({"query": query})
    print("ğŸ’¡ Answer:", answer["result"])
